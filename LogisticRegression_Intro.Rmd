---
title: "LogisticRegression_Intro"
author: "Dr.Sampath"
date: "Created February 16, 2021; updated `r format(Sys.time(), '%B %d, %Y at %H:%M:%S')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(ISLR)
head(Default)
```


```{r}
dim(Default)
```

```{r}
# To have same output
set.seed(42)
# Indexing the sample data
default_idx = sample(nrow(Default), 8000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]
dim(default_trn)
```

```{r}
# To work with Linear Regression
default_trn_lm = default_trn
default_tst_lm = default_tst
tail(default_trn_lm)
```

Since linear regression expects a numeric response variable, we coerce the response to be numeric.
```{r}
default_trn_lm$default = as.numeric(default_trn_lm$default) - 1
default_tst_lm$default = as.numeric(default_tst_lm$default) - 1

```

```{r}
library(tidyverse)
default_trn_lm %>% select(default)
```

```{r}
# Modeleing
model_lm = lm(default ~ balance, data = default_trn_lm)
summary(model_lm)
```

```{r}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Linear Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
abline(model_lm, lwd = 3, col = "dodgerblue")
```

```{r}
# Predicted values are <0.5?
all(predict(model_lm) < 0.5)
any(predict(model_lm) < 0)
```

Odds
===
Generally, **odds** are expressed as the ratio of favorable outcomes to unfavorable outcomes.

For example, Suppose you tosses a coin 6 times and you won 2 times and lost remaining times. Then your odds are 2:4.

To calculate the probabilities, we will use the following formulae:

Probability = odds/1+ odds 

To better estimate the probability
\[ p(x)=P(Y=1|X=x)\]
Consider the following function:
\[log\Big({p(x)\over 1-p(x)}\Big)=\beta_0+\beta_1x_1+\cdots+\beta_px_p\]

Then 
\[p(x)={1\over 1+e^{-(\beta_0+\beta_1x_1+\cdots+\beta_px_p)}}\]
\[=\sigma(\beta_0+\beta_1x_1+\cdots+\beta_px_p),\]
where 
\[\sigma(x)={1\over 1+e^{-x}}\]

- The function $\sigma$ is known as sigmoid function.

- It takes any real input, and outputs a number between 0 and 1.

- 


```{r}
# Basic syntax for logistic regression
model_glm = glm(default ~ balance, data = default_trn, family = "binomial")
summary(model_glm)
```
The model says that 

\[p(x) = {1\over 1+e^{-(10.81+0.0056*balance)}} \]

```{r}
# predict function
head(predict(model_glm))
head(predict(model_glm, type = "response")) # default type = "link"
```

```{r}
# For classification
model_glm_pred = ifelse(predict(model_glm, type = "link") > 0, "Yes", "No")
model_glm_pred = ifelse(predict(model_glm, type = "response") > 0.5, "Yes", "No")
```

```{r}
# Calculate accurary
calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}
```

```{r}
calc_class_err(actual = default_trn$default, predicted = model_glm_pred)
```


```{r}
# Confusion matrix
train_tab = table(predicted = model_glm_pred, actual = default_trn$default)
train_tab
```

- **Sensitivity** (True Positive (TP) rate)  - measures the proportion of positives that are correctly identified 

- **Specificity** (True Negative (TN) rate) - measures the proportion of negatives that are correctly identified 
- The terms “**true positive**”, “**false positive**”, “**true negative**”, and “**false negative**” refer to the result of a test and the correctness of the classification.

- For instance, if the condition is a disease, “true positive” means “*correctly diagnosed as diseased*”, “*false positive*” means “*incorrectly diagnosed as diseased*”, “true negative” means “*correctly diagnosed as not diseased*”, and “*false negative*” means “*incorrectly diagnosed as not diseased*”. 

- If a test's sensitivity is $98\%$ and its specificity is $92\%$, its rate of false negatives is $2\%$ and its rate of false positives is $8\%.$

 | | Diseases (+)|Not Diseased (-) |
  ---:|---: | :--- |
 |Correctly Diagnosed (T) | True Positive | True Negative |
| Incorrectly Diagnosed (F)|  False Positive |False Negative |
 
- In a diagnostic test, sensitivity is a measure of how well a test can identify true positives. Sensitivity can also be referred to as the recall, hit rate, or true positive rate. 

- In a diagnostic test, specificity is a measure of how well a test can identify true negatives. Specificity is also referred to as selectivity or true negative rate.

- In a "good" diagnostic test, the false positives should be very low. That is, people who are identified as having a condition should be highly likely to truly have the condition.

```{r echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("D:/MDSC-206/Images/table1.png")
```

```{r echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("D:/MDSC-206/Images/table2.png")
```




```{r}
library(caret)
train_con_mat = confusionMatrix(train_tab, positive = "Yes")
c(train_con_mat$overall["Accuracy"], 
  train_con_mat$byClass["Sensitivity"], 
  train_con_mat$byClass["Specificity"])
```

```{r}
# Calculate the following
sum(diag(train_tab))/sum(train_tab) # Find the accuracy
7691/(7691+37) # Specificity
86/(186+86) # Sensitivity
```

Column1  | Column 2 
 -------: |:--------   
 Correct | FT      
 TN      | Correct 
```{r}
train_con_mat
```



```{r}
get_logistic_error = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  preds = ifelse(probs > cut, pos, neg)
  calc_class_err(actual = data[, res], predicted = preds)
}
```
```{r}
get_logistic_error(model_glm, data = default_trn, 
                   res = "default", pos = "Yes", neg = "No", cut = 0.5)
```


```{r}
# PLotting
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Logistic Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
curve(predict(model_glm, data.frame(balance = x), type = "response"), 
      add = TRUE, lwd = 3, col = "dodgerblue")
abline(v = -coef(model_glm)[1] / coef(model_glm)[2], lwd = 2)
```

The plot has the following inference:

- The orange | characters are the data $(x_i,y_i)$

- The blue “curve” is the predicted probabilities given by the fitted logistic regression. \[\hat{p}(x)=\hat{P}(Y=1|X=x)\]

- The solid vertical black line represents the **decision** boundary, the `balance` that obtains a predicted probability of **0.5**. In this case `balance` = 1934.2247145.

```{r}
#grid = seq(0, max(default_trn$balance), by = 0.01)

#sigmoid = function(x) {
#  1 / (1 + exp(-x))
#}

#lines(grid, sigmoid(coef(model_glm)[1] + coef(model_glm)[2] * grid), lwd = 3)
```

```{r}
model_1 = glm(default ~ 1, data = default_trn, family = "binomial")
model_2 = glm(default ~ ., data = default_trn, family = "binomial")
model_3 = glm(default ~ . ^ 2 + I(balance ^ 2),
              data = default_trn, family = "binomial")
```

Note that, using polynomial transformations of predictors will allow a linear model to have non-linear decision boundaries.
```{r}
model_list = list(model_1, model_2, model_3)
train_errors = sapply(model_list, get_logistic_error, data = default_trn, 
                      res = "default", pos = "Yes", neg = "No", cut = 0.5)
test_errors  = sapply(model_list, get_logistic_error, data = default_tst, 
                      res = "default", pos = "Yes", neg = "No", cut = 0.5)
```


```{r}
diff(train_errors)
diff(test_errors)
```


ROC CURVES
===

```{r}
model_glm = glm(default ~ balance, data = default_trn, family = "binomial")
probs <- predict(model_glm, newdata = default_tst, type = "response")

```


```{r}
get_logistic_pred = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}
```


```{r}
test_pred_10 = get_logistic_pred(model_glm, data = default_tst,
                                 res = "default", pos = "Yes", neg = "No", cut = 0.1)
test_pred_20 = get_logistic_pred(model_glm, data = default_tst,
                                 res = "default", pos = "Yes", neg = "No", cut = 0.2)
test_pred_30 = get_logistic_pred(model_glm, data = default_tst,
                                 res = "default",pos = "Yes", neg = "No", cut = 0.3)

test_pred_50 = get_logistic_pred(model_glm, data = default_tst,
                                 res = "default",pos = "Yes", neg = "No", cut = 0.5)
test_pred_90 = get_logistic_pred(model_glm, data = default_tst,
                                 res = "default",pos = "Yes", neg = "No", cut = 0.9)
```


Now we evaluate accuracy, sensitivity, and specificity for these classifiers.

```{r}
test_tab_10 = table(predicted = test_pred_10, actual = default_tst$default)
test_tab_20 = table(predicted = test_pred_20, actual = default_tst$default)
test_tab_30 = table(predicted = test_pred_30, actual = default_tst$default)
test_tab_50 = table(predicted = test_pred_50, actual = default_tst$default)
test_tab_90 = table(predicted = test_pred_90, actual = default_tst$default)

test_con_mat_10 = confusionMatrix(test_tab_10, positive = "Yes")
test_con_mat_50 = confusionMatrix(test_tab_50, positive = "Yes")
test_con_mat_90 = confusionMatrix(test_tab_90, positive = "Yes")
```

```{r}
metrics = rbind(
  
  c(test_con_mat_10$overall["Accuracy"], 
    test_con_mat_10$byClass["Sensitivity"], 
    test_con_mat_10$byClass["Specificity"]),
  
  c(test_con_mat_50$overall["Accuracy"], 
    test_con_mat_50$byClass["Sensitivity"], 
    test_con_mat_50$byClass["Specificity"]),
  
  c(test_con_mat_90$overall["Accuracy"], 
    test_con_mat_90$byClass["Sensitivity"], 
    test_con_mat_90$byClass["Specificity"])

)

rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
metrics
```


Note that usually the best accuracy will be seen near  c=0.50.
 

Instead of manually checking cutoffs, we can create an ROC curve (receiver operating characteristic curve) which will sweep through all possible cutoffs, and plot the sensitivity and specificity.

```{r}
library(InformationValue)
opt <- optimalCutoff(default_tst$default,probs)
opt
```











