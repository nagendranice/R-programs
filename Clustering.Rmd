---
title: "Clustering"
author: "Dr.Sampath"
date: "08/04/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
k-means
===

```{r}
set.seed(123)
DATA <- iris
DATA$Species <- NULL
```


```{r}
# ?apply()
# apply(DATA,mean)
# Determine number of clusters
wss <- (nrow(DATA)-1)*sum(apply(DATA,2,var))
for(i in 2:15)wss[i] <- sum(kmeans(DATA, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")
```

```{r}
kmeans3 <- kmeans(DATA, 3)
kmeans3
kmeans2 <- kmeans(DATA, 2)
kmeans2
```


```{r}
table(iris$Species, kmeans3$cluster)
table(iris$Species, kmeans2$cluster)
```

```{r}
par(mfrow=c(1,2))
plot(DATA[c("Sepal.Length", "Sepal.Width")], col = kmeans2$cluster)
points(kmeans2$centers[,c("Sepal.Length", "Sepal.Width")], col = 1:2,pch = 8, cex=2)
plot(DATA[c("Sepal.Length", "Sepal.Width")], col = kmeans2$cluster)
points(kmeans2$centers[,c("Sepal.Length", "Sepal.Width")], col = 1:2,pch = 8, cex=2)
```


```{r}
 plot (DATA , col =(kmeans2$cluster +1) , main =" K - Means Clustering
Results with K = 2" ,pch =20 , cex =2)
```


```{r}
kmeans2 <- kmeans(DATA, 2) # suggested the number of clusters
plot (DATA , col =(kmeans2$cluster +1) , main =" K - Means Clustering
Results with K =3" ,pch =20 , cex =2)
```

```{r}
idx <- sample(1:dim(iris)[1], 25)
irisSample <- iris[idx,]
irisSample$Species <- NULL
#Hierarchical clustering
# Ward Hierarchical Clustering
d <- dist(irisSample, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward")
plot(fit, hang = -1, labels=iris$Species[idx])
 # cut tree into 3 clusters
rect.hclust(fit, k=3)
groups <- cutree(fit, k=3)
```


```{r}
# complete, average and single
hc.complete = hclust(dist(irisSample), method ="complete")
hc.average = hclust(dist(irisSample), method ="average")
hc.single = hclust(dist(irisSample), method ="single")
```



```{r}
par(mfrow = c(1,3))
plot(hc.complete, main =" Complete Linkage " , cex =.9,labels=iris$Species[idx])
rect.hclust(hc.complete, k=3)
groups <- cutree(hc.complete, k=3)
plot(hc.average, main =" Average Linkage " , cex =.9,labels=iris$Species[idx])
rect.hclust(hc.average, k=3)
groups <- cutree(hc.average, k=3)
plot(hc.single, main ="Single Linkage " , cex =.9,labels=iris$Species[idx])
rect.hclust(hc.single, k=3)
groups <- cutree(hc.single, k=3)
```

```{r}
set.seed(3)
km.out = kmeans(DATA ,3 , nstart =1)
km.out$tot.withinss
km.out = kmeans (DATA,3 , nstart = 20)
#km.out
km.out$tot.withinss
```


k-medoids
===

The k-medoids clustering is very similar to k-means, and the major difference between them is that: while a cluster is represented with its center in the k-means algorithm, it is represented with the object closest to
the center of the cluster in the k-medoids clustering. The k-medoids clustering is more robust than
k-means in presence of outliers

- PAM (Partitioning Around Medoids)
- CLARA

```{r}
library(fpc)
library(cluster)
pamk.result <- pamk(DATA)
# number of clusters
pamk.result$nc
clara.result <-  clara(DATA,k=3)
clara.result
table(clara.result$clustering, iris$Species)
```


```{r}
 # check clustering against actual species
table(pamk.result$pamobject$clustering, iris$Species)
```

```{r}
pam.result <- pam(DATA, 3)
table(pam.result$clustering, iris$Species)
```

# Clustering

- k-means
- k-medoids
  - pam
  - pamk
  - clara (You can explore about this)
- Hierarchical Cluster

Detecting Outliers
===

```{r}
set.seed(3147)
x <- rnorm(100)
summary(x)
```


```{r}
# outliers
boxplot.stats(x)$out
boxplot(x)
```

```{r}
y <- rnorm(100)
df <- data.frame(x, y)
head(df)
```


```{r}
# find the index of outliers from x
(a <- which(x %in% boxplot.stats(x)$out))
# find the index of outliers from y
(b <- which(y %in% boxplot.stats(y)$out))
# outliers in both x and y
(outlier.list1 <- intersect(a,b))
```


```{r}
plot(df)
points(df[outlier.list1,], col="red", pch="+", cex=2.5)
```


```{r}
# outliers in either x or y
(outlier.list2 <- union(a,b))
plot(df)
points(df[outlier.list2,], col="blue", pch="x", cex=2)
```


```{r}
# Detecting outliers using PCA
n <- nrow(DATA)
labels <- 1:n
#labels[-outliers] <- "."
biplot(prcomp(DATA), cex=.8, xlabs=labels)
```


```{r}
# Detecting outliers using clustering
kmeans3$centers
# cluster IDs
kmeans3$cluster
# calculate distances between objects and cluster centers
centers <- kmeans3$centers[kmeans3$cluster, ]
distances <- sqrt(rowSums((DATA - centers)^2))
# pick top 5 largest distances
outliers <- order(distances, decreasing=T)[1:5]
# who are outliers
print(outliers)
```

```{r}
print(DATA[outliers,])
```

```{r}
# plot clusters
plot(DATA[,c("Sepal.Length", "Sepal.Width")], pch="o",col=kmeans3$cluster, cex=1.5)
# plot cluster centers
points(kmeans3$centers[,c("Sepal.Length", "Sepal.Width")], col=1:3, pch=8, cex=1.5)
# plot outliers
points(DATA[outliers, c("Sepal.Length", "Sepal.Width")], pch="+", col=4, cex=1.5)
```

Multinomial Logistic Regression
===

- softmax


```{r}
#Splitting the data using a function from dplyr package
library(caret)

index <- createDataPartition(iris$Species, p = .60, list = FALSE)
train <- iris[index,]
test <- iris[-index,]
```


```{r}
require(nnet)
# Training the multinomial model
multinom_model <- multinom(Species ~ ., data = train)

# Checking the model
summary(multinom_model)
```

Predicting and Validating

```{r}
# Predicting the values for train dataset
train$ClassPredicted <- predict(multinom_model, newdata = train, "class")

# Building classification table
tab <- table(train$Species, train$ClassPredicted)

# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab))/sum(tab))*100,2)
```



```{r}
tab
```


```{r}
test$ClassPredicted <- predict(multinom_model, newdata = test, "class")
tab_test <- table(test$Species, test$ClassPredicted)
tab_test
round((sum(diag(tab_test))/sum(tab_test))*100,2)
```


Questions:

-Dataset: Penguins
-Cluster Analysis
- Outliers Detection: Box-plot, PCA, Clustering
- Multinomial class: Compare this results with LDA and QDA
-


